{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e867d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38416bb5",
   "metadata": {},
   "source": [
    "### Importing required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b8251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c6cd05",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61953c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>edgar/data/3662/0000950170-98-000413.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>edgar/data/3662/0000950170-98-001001.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-000783.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>edgar/data/3662/0000950170-98-002145.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>edgar/data/3662/0000950172-98-001203.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                   SECFNAME  \n",
       "0  edgar/data/3662/0000950170-98-000413.txt  \n",
       "1  edgar/data/3662/0000950170-98-001001.txt  \n",
       "2  edgar/data/3662/0000950172-98-000783.txt  \n",
       "3  edgar/data/3662/0000950170-98-002145.txt  \n",
       "4  edgar/data/3662/0000950172-98-001203.txt  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_excel('cik_list.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60c39a4",
   "metadata": {},
   "source": [
    "### loading the master dictionary so that we can extract postive and negative word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ad2cdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_dic = pd.read_excel('LoughranMcDonald_MasterDictionary_2018.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff45caa",
   "metadata": {},
   "source": [
    "### Adding the 'https://www.sec.gov/Archives/' in the SECFNAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7a1de01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CIK</th>\n",
       "      <th>CONAME</th>\n",
       "      <th>FYRMO</th>\n",
       "      <th>FDATE</th>\n",
       "      <th>FORM</th>\n",
       "      <th>SECFNAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199803</td>\n",
       "      <td>1998-03-06</td>\n",
       "      <td>10-K405</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199805</td>\n",
       "      <td>1998-05-15</td>\n",
       "      <td>10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199808</td>\n",
       "      <td>1998-08-13</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-12</td>\n",
       "      <td>10-K/A</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3662</td>\n",
       "      <td>SUNBEAM CORP/FL/</td>\n",
       "      <td>199811</td>\n",
       "      <td>1998-11-16</td>\n",
       "      <td>NT 10-Q</td>\n",
       "      <td>https://www.sec.gov/Archives/edgar/data/3662/0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CIK            CONAME   FYRMO      FDATE     FORM  \\\n",
       "0  3662  SUNBEAM CORP/FL/  199803 1998-03-06  10-K405   \n",
       "1  3662  SUNBEAM CORP/FL/  199805 1998-05-15     10-Q   \n",
       "2  3662  SUNBEAM CORP/FL/  199808 1998-08-13  NT 10-Q   \n",
       "3  3662  SUNBEAM CORP/FL/  199811 1998-11-12   10-K/A   \n",
       "4  3662  SUNBEAM CORP/FL/  199811 1998-11-16  NT 10-Q   \n",
       "\n",
       "                                            SECFNAME  \n",
       "0  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "1  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "2  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "3  https://www.sec.gov/Archives/edgar/data/3662/0...  \n",
       "4  https://www.sec.gov/Archives/edgar/data/3662/0...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SECFNAME'] ='https://www.sec.gov/Archives/' + df['SECFNAME'].astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956eccb1",
   "metadata": {},
   "source": [
    "### getting all the links in new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd2a46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = df['SECFNAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1b0c02",
   "metadata": {},
   "source": [
    "### putting text of the all 152 file into reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf70cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = []\n",
    "for url in links:\n",
    "    r = requests.get(url, headers={'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.152 Safari/537.36'})\n",
    "    data = r.text\n",
    "    soup = BeautifulSoup(data, \"html.parser\")\n",
    "    reports.append(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5418608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "print(len(reports))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b848b721",
   "metadata": {},
   "source": [
    "### #Loading the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e203ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words1=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24af09",
   "metadata": {},
   "source": [
    "### Extracting the postive and negative word from master dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af69fdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_dictionary = [x.lower() for x in master_dic[master_dic['Positive'] != 0]['Word']]\n",
    "negative_dictionary = [x.lower() for x in master_dic[master_dic['Negative'] != 0]['Word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "050632d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "354\n",
      "2355\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_dictionary))\n",
    "print(len(negative_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "772fac57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['able', 'abundance', 'abundant', 'acclaimed', 'accomplish']\n",
      "['abandon', 'abandoned', 'abandoning', 'abandonment', 'abandonments']\n"
     ]
    }
   ],
   "source": [
    "print(positive_dictionary[:5])\n",
    "print(negative_dictionary[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29917f45",
   "metadata": {},
   "source": [
    "### loading the uncertinity and constraing word dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebadcfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncertainity = pd.read_excel('uncertainty_dictionary.xlsx')\n",
    "#uncertainity_words = list(uncertainity['Word'])\n",
    "uncertainity_words=[x.lower() for x in list(uncertainity['Word'])]\n",
    "\n",
    "constraining = pd.read_excel('constraining_dictionary.xlsx')\n",
    "constraining_words=[x.lower() for x in list(constraining['Word'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09af4642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "print(len(uncertainity_words))\n",
    "print(len(constraining_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d459069a",
   "metadata": {},
   "source": [
    "###  writing the function for tokenize, stop word removal, polarity, syllable count, fog index\n",
    "### we are going to use it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7653b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = re.sub(r'[^A-Za-z]',' ',text.lower())\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    return tokenized_words\n",
    "\n",
    "def remove_stopwords1(words, stop_words1):\n",
    "    return [x for x in words if x not in stop_words1]\n",
    "    \n",
    "def polarity(positive_score, negative_score):\n",
    "     return (positive_score - negative_score)/((positive_score + negative_score)+ 0.000001)\n",
    "def avgsentlength(total_word, sentence_legnth):\n",
    "     return total_word/sentence_legnth\n",
    "     \n",
    "def syllable_morethan2(word):\n",
    "    if(len(word) > 2 and (word[-2:] == 'es' or word[-2:] == 'ed')):\n",
    "        return False\n",
    "    \n",
    "    count =0\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for i in word:\n",
    "        if(i.lower() in vowels):\n",
    "            count = count +1\n",
    "        \n",
    "    if(count > 2):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def fog_index_cal(average_sentence_length, percentage_complexwords):\n",
    "    return 0.4*(average_sentence_length + percentage_complexwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478f472",
   "metadata": {},
   "source": [
    "### defining one main function and calling tokenize and remove stop word funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87720962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(reports):\n",
    "    \n",
    "    doc=list(tokenize(str(reports)))\n",
    "    doc1=remove_stopwords1(doc,stop_words1)\n",
    "    return doc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e01707",
   "metadata": {},
   "source": [
    "### storing the word of all 152 documents into empty list. \n",
    "### The empty list is a nested lsited each list within list contains the word of each documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1da0a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty=[]\n",
    "for i in reports:\n",
    "    result = main(i)\n",
    "    empty.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47cb9bc",
   "metadata": {},
   "source": [
    "### calculating the word length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "217b57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_length=[len(empty[i]) for i in range(len(empty))]\n",
    "df['word_count'] = word_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b180685",
   "metadata": {},
   "source": [
    "### calculating postive and negative score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5328b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_score=[]\n",
    "neg_score=[]\n",
    "for x in empty:\n",
    "    negative_score = 0    \n",
    "    positive_score = 0\n",
    "    for d in x:\n",
    "        if(d in negative_dictionary):\n",
    "              negative_score = negative_score+1\n",
    "                \n",
    "        if(d in positive_dictionary):\n",
    "              positive_score = positive_score+1\n",
    "    neg_score.append(negative_score)\n",
    "    pos_score.append(positive_score)\n",
    "    \n",
    "df['negative_score'] = pos_score\n",
    "df['positive_score'] = neg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e6483b",
   "metadata": {},
   "source": [
    "### calculating polarity_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31bfa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "polarity_point=[polarity(x,y) for x, y in zip(pos_score, neg_score)]\n",
    "df['polarity_point'] = polarity_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e58643",
   "metadata": {},
   "source": [
    "### calculating average sentence lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30508bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence tokenize\n",
    "total_sent1=[]\n",
    "for x in reports:\n",
    "    total_sent=len(sent_tokenize(x))\n",
    "    total_sent1.append(total_sent)\n",
    "    \n",
    "Avg_sent_lenght=[avgsentlength(x,y) for x, y in zip(word_length, total_sent1)]\n",
    "    \n",
    "df['Avg_sent_lenght'] = Avg_sent_lenght"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb4e34",
   "metadata": {},
   "source": [
    "### calculating No. of complex word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8f73d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "complex_word_list=[]\n",
    "\n",
    "for x in empty:\n",
    "    num_complexword =0\n",
    "    for d in x:\n",
    "        if(syllable_morethan2(d)):\n",
    "            num_complexword = num_complexword+1\n",
    "    complex_word_list.append(num_complexword)\n",
    "     \n",
    "df['complex_word_count'] = complex_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fa642",
   "metadata": {},
   "source": [
    "### calculating percentage of complex word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "235cdbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_complex_word=[x/y for x, y in zip(complex_word_list, word_length)]\n",
    "df['percentage_of_complex_words'] = percentage_complex_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33828a",
   "metadata": {},
   "source": [
    "### calculating uncertainty_score and constraining_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc722139",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncer=[]\n",
    "constr=[]\n",
    "for x in empty:\n",
    "    uncertainity_score = 0\n",
    "    constraining_score = 0\n",
    "    for d in x:\n",
    "         if(d in uncertainity_words):\n",
    "             uncertainity_score = uncertainity_score+1\n",
    "\n",
    "         if(d in constraining_words):\n",
    "             constraining_score = constraining_score+1\n",
    "                \n",
    "    uncer.append(uncertainity_score)\n",
    "    constr.append(constraining_score)\n",
    "df['uncertainty_score'] = uncer\n",
    "df['constraining_score'] = constr          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b694cb6e",
   "metadata": {},
   "source": [
    "### calculating Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7342ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fog_Score=[fog_index_cal(x,y) for x, y in zip(Avg_sent_lenght, percentage_complex_word)]\n",
    "df['fog_index'] = fog_Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6771bcef",
   "metadata": {},
   "source": [
    "### calculating the word proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "936ecad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## positive_word_proportion\n",
    "\n",
    "positive_word_proportion=[x/y for x, y in zip(pos_score, word_length)]\n",
    "df['positive_word_proportion'] = positive_word_proportion\n",
    "\n",
    "## negative_word_proportion\n",
    "negative_word_proportion=[x/y for x, y in zip(neg_score, word_length)]\n",
    "df['negative_word_proportion'] = negative_word_proportion\n",
    "\n",
    "## uncertainity_word_proportion\n",
    "uncertainity_word_proportion=[x/y for x, y in zip(uncer, word_length)]\n",
    "df['uncertainity_word_proportion'] = uncertainity_word_proportion\n",
    "\n",
    "## constraining_word_proportion\n",
    "\n",
    "constraining_word_proportion=[x/y for x, y in zip(constr, word_length)]\n",
    "df['constraining_word_proportion'] = constraining_word_proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df6db8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "### calculating the constraining_words_whole_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5004dd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2.252099e-04\n",
       "1      1.584194e-04\n",
       "2      7.572628e-07\n",
       "3      1.084400e-04\n",
       "4      6.058102e-07\n",
       "           ...     \n",
       "147    2.211207e-05\n",
       "148    4.543577e-07\n",
       "149    3.786314e-06\n",
       "150    2.196062e-05\n",
       "151    5.906649e-06\n",
       "Name: constraining_words_whole_report, Length: 152, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_of_total_word=sum(word_length)\n",
    "sum_of_total_word\n",
    "\n",
    "## constraining_word_proportion\n",
    "\n",
    "constraining_words_whole_report=[x/sum_of_total_word for x in constr]\n",
    "# constraining_words_whole_report=[x/y for x, y in zip(constr, sum_of_total_word)]\n",
    "df['constraining_words_whole_report'] = constraining_words_whole_report\n",
    "df['constraining_words_whole_report'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec76a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Exporting to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d63e132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('blackoffer_output.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
